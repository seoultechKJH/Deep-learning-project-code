{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c84071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from CifarImbalanced import CIFAR10, CIFAR100\n",
    "from FocalLossV5 import FocalLoss\n",
    "\n",
    "from EnsembleV2 import resnext29_16_64, resnext47_16_64, resnext56_16_64\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "def acc(outputs, labels):\n",
    "    _, preds = torch.max(outputs.data, 1) # output 각 이미지마다 가장 큰 확률의 인덱스(=class 번호)를 변수 preds에 저장\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds)) # 분류가 잘된 개수로 정확도 산출\n",
    "\n",
    "def train(model, trainLoader, criterion, optimizer, use_cuda=True, weights_for_classes=None):\n",
    "    model.train()\n",
    "    runningLoss = 0.0\n",
    "    runningCorrects = 0\n",
    "    runningCorrectsEnsembleUnweighted = 0.0\n",
    "\n",
    "    for inputs, labels in trainLoader:\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs, out1x1, out2x1,out3x1,out4x1,out5x1 = model(x=inputs)\n",
    "            acc0 = acc(outputs, labels)\n",
    "            acc1 = acc(out1x1, labels)\n",
    "            acc2 = acc(out2x1, labels)\n",
    "            acc3 = acc(out3x1, labels)\n",
    "            acc4 = acc(out4x1, labels)\n",
    "            acc5 = acc(out5x1, labels)\n",
    "\n",
    "            w_for_classifier_sum = acc0+acc1+acc2+acc3+acc4+acc5\n",
    "            w_for_classifier0 = acc0/w_for_classifier_sum\n",
    "            w_for_classifier1 = acc1/w_for_classifier_sum\n",
    "            w_for_classifier2 = acc2/w_for_classifier_sum\n",
    "            w_for_classifier3 = acc3/w_for_classifier_sum\n",
    "            w_for_classifier4 = acc4/w_for_classifier_sum\n",
    "            w_for_classifier5 = acc5/w_for_classifier_sum\n",
    "\n",
    "            w_for_instances = torch.zeros(outputs.size(0), 1)\n",
    "            if use_cuda:\n",
    "                w_for_instances = w_for_instances.cuda()\n",
    "            #    weights_for_classes = weights_for_classes.cuda()\n",
    "\n",
    "            weights_for_classes = None\n",
    "            \n",
    "            loss1, w_for_instances = criterion(out1x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            loss2, w_for_instances = criterion(out2x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            loss3, w_for_instances = criterion(out3x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            loss4, w_for_instances = criterion(out4x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            loss5, w_for_instances = criterion(out5x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "\n",
    "            loss0, w_for_instances = criterion(outputs, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            \n",
    "            #loss = loss0+0.3*loss1+0.3*loss2+0.3*loss3+0.3*loss4+0.3*loss5\n",
    "            loss = loss0+loss1+loss2+loss3+loss4+loss5\n",
    "\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, preds1 = torch.max(out1x1, 1)\n",
    "            _, preds2 = torch.max(out2x1, 1)\n",
    "            _, preds3 = torch.max(out3x1, 1)\n",
    "            _, preds4 = torch.max(out4x1, 1)\n",
    "            _, preds5 = torch.max(out5x1, 1)\n",
    "            \n",
    "            outputsEnsemble_unweighted = torch.zeros(outputs.size())\n",
    "\n",
    "            for i in range(inputs.size(0)):\n",
    "                outputsEnsemble_unweighted[i, preds[i]] = outputsEnsemble_unweighted[i, preds[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds1[i]] = outputsEnsemble_unweighted[i, preds1[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds2[i]] = outputsEnsemble_unweighted[i, preds2[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds3[i]] = outputsEnsemble_unweighted[i, preds3[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds4[i]] = outputsEnsemble_unweighted[i, preds4[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds5[i]] = outputsEnsemble_unweighted[i, preds5[i]] + 1\n",
    "            \n",
    "            _, predsEnsembleUnweighted = torch.max(outputsEnsemble_unweighted, 1)\n",
    "            \n",
    "            if use_cuda:\n",
    "                runningCorrectsEnsembleUnweighted = runningCorrectsEnsembleUnweighted+ torch.sum(predsEnsembleUnweighted.cuda()==labels.data).cpu()\n",
    "            else:\n",
    "                runningCorrectsEnsembleUnweighted = runningCorrectsEnsembleUnweighted+ torch.sum(predsEnsembleUnweighted==labels.data)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "        runningLoss += loss.item() * inputs.size(0)\n",
    "        runningCorrects = runningCorrects + torch.sum(preds == labels.data)\n",
    "\n",
    "        \n",
    "        overallLoss = [w_for_classifier0,w_for_classifier1,w_for_classifier2,w_for_classifier3,w_for_classifier4,w_for_classifier5]\n",
    "        if use_cuda:\n",
    "            model.module.setWeights(overallLoss)\n",
    "        else:\n",
    "            model.setWeights(overallLoss)\n",
    "    \n",
    "    epochLoss = runningLoss / len(trainLoader.dataset)\n",
    "    epochAcc = runningCorrects.double() / len(trainLoader.dataset)\n",
    "    epochAccEnsembleUnweighted = runningCorrectsEnsembleUnweighted.double() / len(trainLoader.dataset)\n",
    "    print('Train Loss: {:.4f} Acc:{:.4f} EnsembleAcc:{:.4f}'.format(epochLoss, epochAcc, epochAccEnsembleUnweighted))\n",
    "    return (epochLoss, epochAcc, epochAccEnsembleUnweighted)\n",
    "\n",
    "def validate(model, testLoader, criterion, optimizer, use_cuda=True):\n",
    "    #print('test on validation')\n",
    "    model.eval()\n",
    "    runningLoss = 0.0\n",
    "    runningCorrects = 0.0\n",
    "   \n",
    "    runningCorrectsEnsemble=0.0\n",
    "    runningCorrectsEnsembleUnweighted = 0.0\n",
    "    runningCorrectsEnsemble_plus = 0.0\n",
    "    runningCorrectsEnsembleUnweighted_plus = 0.0\n",
    "    if use_cuda:\n",
    "        w = model.module.getWeights()\n",
    "    else:\n",
    "        w = model.getWeights()\n",
    "    #w = [1,1,1,1,1,1]\n",
    "    for inputs, labels in testLoader:\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            optimizer.zero_grad()\n",
    "            outputs,out1x1,out2x1,out3x1,out4x1,out5x1 = model(x=inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, preds1 = torch.max(out1x1, 1)\n",
    "            _, preds2 = torch.max(out2x1, 1)\n",
    "            _, preds3 = torch.max(out3x1, 1)\n",
    "            _, preds4 = torch.max(out4x1, 1)\n",
    "            _, preds5 = torch.max(out5x1, 1)\n",
    "            \n",
    "            outputsEnsemble = torch.zeros(outputs.size())\n",
    "            outputsEnsemble_unweighted = torch.zeros(outputs.size())\n",
    "\n",
    "            outputEnsemble_plus = outputs*w[0]+out1x1*w[1]+out2x1*w[2]+out3x1*w[3]+out4x1*w[4]+out5x1*w[5]\n",
    "            outputEnsemble_unweighted_plus = outputs+out1x1+out2x1+out3x1+out4x1+out5x1\n",
    "\n",
    "            for i in range(inputs.size(0)):\n",
    "                outputsEnsemble[i, preds[i]] = outputsEnsemble[i, preds[i]] + w[0]\n",
    "                outputsEnsemble[i, preds1[i]] = outputsEnsemble[i, preds1[i]] + w[1]\n",
    "                outputsEnsemble[i, preds2[i]] = outputsEnsemble[i, preds2[i]] + w[2]\n",
    "                outputsEnsemble[i, preds3[i]] = outputsEnsemble[i, preds3[i]] + w[3]\n",
    "                outputsEnsemble[i, preds4[i]] = outputsEnsemble[i, preds4[i]] + w[4]\n",
    "                outputsEnsemble[i, preds5[i]] = outputsEnsemble[i, preds5[i]] + w[5]\n",
    "\n",
    "                outputsEnsemble_unweighted[i, preds[i]] = outputsEnsemble_unweighted[i, preds[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds1[i]] = outputsEnsemble_unweighted[i, preds1[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds2[i]] = outputsEnsemble_unweighted[i, preds2[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds3[i]] = outputsEnsemble_unweighted[i, preds3[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds4[i]] = outputsEnsemble_unweighted[i, preds4[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds5[i]] = outputsEnsemble_unweighted[i, preds5[i]] + 1\n",
    "            \n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, predsEnsemble = torch.max(outputsEnsemble, 1)\n",
    "            _, predsEnsembleUnweighted = torch.max(outputsEnsemble_unweighted, 1)\n",
    "            _, predsEnsemble_plus = torch.max(outputEnsemble_plus, 1)\n",
    "            _, predsEnsembleUnweighted_plus = torch.max(outputEnsemble_unweighted_plus, 1)\n",
    "            \n",
    "            if use_cuda:\n",
    "                runningCorrects = runningCorrects + torch.sum(preds==labels.data).cpu()\n",
    "                runningCorrectsEnsemble = runningCorrectsEnsemble + torch.sum(predsEnsemble.cuda()==labels.data).cpu()\n",
    "                runningCorrectsEnsembleUnweighted = runningCorrectsEnsembleUnweighted+ torch.sum(predsEnsembleUnweighted.cuda()==labels.data).cpu()\n",
    "                runningCorrectsEnsemble_plus = runningCorrectsEnsemble_plus + torch.sum(predsEnsemble_plus.cuda()==labels.data).cpu()\n",
    "                runningCorrectsEnsembleUnweighted_plus = runningCorrectsEnsembleUnweighted_plus+ torch.sum(predsEnsembleUnweighted_plus.cuda()==labels.data).cpu()\n",
    "            else:\n",
    "                runningCorrects = runningCorrects + torch.sum(preds==labels.data)\n",
    "                runningCorrectsEnsemble = runningCorrectsEnsemble + torch.sum(predsEnsemble==labels.data)\n",
    "                runningCorrectsEnsembleUnweighted = runningCorrectsEnsembleUnweighted+ torch.sum(predsEnsembleUnweighted==labels.data)\n",
    "                runningCorrectsEnsemble_plus = runningCorrectsEnsemble_plus + torch.sum(predsEnsemble_plus==labels.data)\n",
    "                runningCorrectsEnsembleUnweighted_plus = runningCorrectsEnsembleUnweighted_plus+ torch.sum(predsEnsembleUnweighted_plus==labels.data)\n",
    "            \n",
    "    \n",
    "    epochAcc = runningCorrects.double() / len(testLoader.dataset)\n",
    "    epochAccEnsemble = runningCorrectsEnsemble.double() / len(testLoader.dataset)\n",
    "    epochAccEnsembleUnweighted = runningCorrectsEnsembleUnweighted.double() / len(testLoader.dataset)\n",
    "    epochAccEnsemble_plus = runningCorrectsEnsemble_plus.double() / len(testLoader.dataset)\n",
    "    epochAccEnsembleUnweighted_plus = runningCorrectsEnsembleUnweighted_plus.double() / len(testLoader.dataset)\n",
    "    \n",
    "    print('Acc:{:.4f}   EnsembleAcc:{:.4f}   EnsembleUnweightedAcc:{:.4f}   Ensemble_plus:{:.4f}  Ensemble_unweighted:{:.4f}'.format(epochAcc, epochAccEnsemble, epochAccEnsembleUnweighted, epochAccEnsemble_plus, epochAccEnsembleUnweighted_plus))\n",
    "    \n",
    "    return ( epochAcc, epochAccEnsembleUnweighted )\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', acc=100., epoch=1):\n",
    "    if os.path.exists(checkpoint)==False:\n",
    "        os.makedirs(checkpoint)\n",
    "\n",
    "    filename = \"Epoch_\"+str(epoch)+\"_\"+str(acc)+\"_checkpoint.pth.tar\"\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, lr, epoch, schedule):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 following schedule\"\"\"\n",
    "    if epoch in schedule:\n",
    "        lr *= 0.1\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def get_checkpoint(epoch, checkpoint='checkpoint'):\n",
    "    if os.path.exists(checkpoint)==False:\n",
    "        os.makedirs(checkpoint)\n",
    "    \n",
    "    filename = \"Epoch_\"+str(epoch)+\"_checkpoint.pth.tar\"\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    return filepath\n",
    "\n",
    "def MyEnsemble(IR=10., Cifar10_used=False, step_imbalance=True, root=\"./Result\", fine_tunning = 0):\n",
    "    if Cifar10_used:\n",
    "        num_class = 10\n",
    "    else:\n",
    "        num_class = 100\n",
    "    \n",
    "    learningRate = 0.05\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "    batchSize = 100 #원래는 100인데 수정\n",
    "\n",
    "    start_epoch = 0\n",
    "    epochs = 100 #원래는 100인데 수정\n",
    "    schedule = [30, 60, 90, 100]\n",
    "    gamma = 2.0\n",
    "    if step_imbalance:\n",
    "        model_save_path = os.path.join(root, \"step_imbalance\", \"cifar\"+str(num_class), \"IR=\"+str(IR), 'MyEnsemble')\n",
    "    else:\n",
    "        model_save_path = os.path.join(root, 'long_tail', \"cifar\"+str(num_class), \"IR=\"+str(IR), str(gamma), 'MyEnsemble')\n",
    "    \n",
    "    best_prec1 = 0\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # create model\n",
    "    model = resnext47_16_64(num_classes=num_class)\n",
    "    criterion = FocalLoss()\n",
    "    \n",
    "    if use_cuda:\n",
    "        #model.cuda()\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learningRate,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    transformTrain = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    transformTest = transforms.Compose([\n",
    "            transforms.CenterCrop(32),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    if Cifar10_used:\n",
    "        trainSet = CIFAR10(root='./data', train=True, download=True, transform=transformTrain, step_imbalance=step_imbalance, IR=IR)\n",
    "        trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize,  shuffle=True, num_workers=4)\n",
    "        testSet = CIFAR10(root='./data', train=False, download=True, transform=transformTest)\n",
    "        testLoader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=True, num_workers=4)\n",
    "        weights_for_classes = trainSet.weight_for_classes\n",
    "    else:\n",
    "        trainSet = CIFAR100(root='./data', train=True, download=True, transform=transformTrain, step_imbalance=step_imbalance, IR=IR)\n",
    "        trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize,  shuffle=True, num_workers=4)\n",
    "        testSet = CIFAR100(root='./data', train=False, download=True, transform=transformTest)\n",
    "        testLoader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=True, num_workers=4)\n",
    "        weights_for_classes = trainSet.weight_for_classes\n",
    "    if fine_tunning>0:\n",
    "        resume = get_checkpoint(fine_tunning, model_save_path)\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(resume, checkpoint['epoch']))\n",
    "            checkpoint = os.path.dirname(resume)\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "    \n",
    "    singleTrainAcc = []\n",
    "    singleTestAcc = []\n",
    "    ensembleTrainAcc = []\n",
    "    ensembleTestAcc = []\n",
    "    #training & evaluation\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        learningRate = adjust_learning_rate(optimizer, learningRate, epoch, schedule)\n",
    "        print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, learningRate))\n",
    "\n",
    "        # train for one epoch\n",
    "        train_loss, train_acc, ensembleAcc = train(model, trainLoader, criterion, optimizer, use_cuda=use_cuda, weights_for_classes=weights_for_classes)\n",
    "        singleAcc, prec1 = validate(model, testLoader, criterion, optimizer, use_cuda=use_cuda)\n",
    "\n",
    "        singleTrainAcc.append(train_acc.cpu().numpy())\n",
    "        singleTestAcc.append(singleAcc.cpu().numpy())\n",
    "        ensembleTrainAcc.append(ensembleAcc.cpu().numpy())\n",
    "        ensembleTestAcc.append(prec1.cpu().numpy())\n",
    "\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        if (epoch % 5)==0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': prec1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best, checkpoint=model_save_path, acc=prec1.cpu().numpy(), epoch=epoch+1)\n",
    "\n",
    "    print('Best accuracy:')\n",
    "    print(best_prec1)\n",
    "    print(singleTrainAcc)\n",
    "    print(singleTestAcc)\n",
    "    print(ensembleTrainAcc)\n",
    "    print(ensembleTestAcc)\n",
    "\n",
    "    return best_prec1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1557a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5eb62231e9a4be4b91018f82208e974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1___27547\n",
      "[500, 496, 491, 487, 482, 478, 473, 469, 464, 460, 455, 450, 446, 441, 437, 432, 428, 423, 419, 414, 410, 405, 400, 396, 391, 387, 382, 378, 373, 369, 364, 360, 355, 351, 346, 341, 337, 332, 328, 323, 319, 314, 310, 305, 300, 296, 291, 287, 282, 278, 273, 269, 264, 260, 255, 250, 246, 241, 237, 232, 228, 223, 219, 214, 210, 205, 201, 196, 191, 187, 182, 178, 173, 169, 164, 160, 155, 150, 146, 141, 137, 132, 128, 123, 119, 114, 110, 105, 100, 96, 91, 87, 82, 78, 73, 69, 64, 60, 55, 50]\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: [1 | 100] LR: 0.050000\n",
      "Train Loss: 24.7521 Acc:0.0734 EnsembleAcc:0.0745\n",
      "Acc:0.0859   EnsembleAcc:0.0824   EnsembleUnweightedAcc:0.0807   Ensemble_plus:0.0846  Ensemble_unweighted:0.0843\n",
      "\n",
      "Epoch: [2 | 100] LR: 0.050000\n",
      "Train Loss: 21.5827 Acc:0.1388 EnsembleAcc:0.1344\n",
      "Acc:0.1291   EnsembleAcc:0.1223   EnsembleUnweightedAcc:0.1160   Ensemble_plus:0.1259  Ensemble_unweighted:0.1250\n",
      "\n",
      "Epoch: [3 | 100] LR: 0.050000\n",
      "Train Loss: 20.0394 Acc:0.1868 EnsembleAcc:0.1798\n",
      "Acc:0.1731   EnsembleAcc:0.1642   EnsembleUnweightedAcc:0.1569   Ensemble_plus:0.1728  Ensemble_unweighted:0.1714\n",
      "\n",
      "Epoch: [4 | 100] LR: 0.050000\n",
      "Train Loss: 18.6687 Acc:0.2356 EnsembleAcc:0.2213\n",
      "Acc:0.2274   EnsembleAcc:0.2289   EnsembleUnweightedAcc:0.2041   Ensemble_plus:0.2255  Ensemble_unweighted:0.2140\n",
      "\n",
      "Epoch: [5 | 100] LR: 0.050000\n",
      "Train Loss: 17.2563 Acc:0.2865 EnsembleAcc:0.2699\n",
      "Acc:0.2390   EnsembleAcc:0.2329   EnsembleUnweightedAcc:0.2113   Ensemble_plus:0.2325  Ensemble_unweighted:0.2243\n",
      "\n",
      "Epoch: [6 | 100] LR: 0.050000\n",
      "Train Loss: 15.9662 Acc:0.3395 EnsembleAcc:0.3155\n",
      "Acc:0.2911   EnsembleAcc:0.2849   EnsembleUnweightedAcc:0.2649   Ensemble_plus:0.2841  Ensemble_unweighted:0.2776\n",
      "\n",
      "Epoch: [7 | 100] LR: 0.050000\n",
      "Train Loss: 14.8439 Acc:0.3812 EnsembleAcc:0.3532\n",
      "Acc:0.2976   EnsembleAcc:0.2894   EnsembleUnweightedAcc:0.2775   Ensemble_plus:0.2935  Ensemble_unweighted:0.2886\n",
      "\n",
      "Epoch: [8 | 100] LR: 0.050000\n",
      "Train Loss: 13.8098 Acc:0.4222 EnsembleAcc:0.3942\n",
      "Acc:0.3443   EnsembleAcc:0.3364   EnsembleUnweightedAcc:0.3103   Ensemble_plus:0.3327  Ensemble_unweighted:0.3249\n",
      "\n",
      "Epoch: [9 | 100] LR: 0.050000\n",
      "Train Loss: 12.8998 Acc:0.4575 EnsembleAcc:0.4319\n",
      "Acc:0.3574   EnsembleAcc:0.3580   EnsembleUnweightedAcc:0.3388   Ensemble_plus:0.3613  Ensemble_unweighted:0.3549\n",
      "\n",
      "Epoch: [10 | 100] LR: 0.050000\n",
      "Train Loss: 12.0027 Acc:0.4983 EnsembleAcc:0.4705\n",
      "Acc:0.4124   EnsembleAcc:0.4107   EnsembleUnweightedAcc:0.3945   Ensemble_plus:0.4153  Ensemble_unweighted:0.4066\n",
      "\n",
      "Epoch: [11 | 100] LR: 0.050000\n",
      "Train Loss: 11.2865 Acc:0.5268 EnsembleAcc:0.5004\n",
      "Acc:0.4123   EnsembleAcc:0.4231   EnsembleUnweightedAcc:0.3973   Ensemble_plus:0.4223  Ensemble_unweighted:0.4117\n",
      "\n",
      "Epoch: [12 | 100] LR: 0.050000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18160/3889879414.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mMyEnsemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18160/203935026.py\u001b[0m in \u001b[0;36mMyEnsemble\u001b[1;34m(IR, Cifar10_used, step_imbalance, root, fine_tunning)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;31m# train for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensembleAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_for_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights_for_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[0msingleAcc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprec1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18160/203935026.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, trainLoader, criterion, optimizer, use_cuda, weights_for_classes)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MyEnsemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db5fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
