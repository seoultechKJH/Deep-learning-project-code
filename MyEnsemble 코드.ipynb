{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c84071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Pytorch 관련 패키지\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from CifarImbalanced import CIFAR10, CIFAR100\n",
    "\n",
    "# 논문에서 제안하는 loss 함수\n",
    "from FocalLossV5 import FocalLoss\n",
    "\n",
    "# 논문에서 제안하는 네트워크\n",
    "from EnsembleV2 import resnext29_16_64, resnext47_16_64, resnext56_16_64\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "# 정확도 계산 함수 정의\n",
    "def acc(outputs, labels):\n",
    "    _, preds = torch.max(outputs.data, 1) # output 각 이미지마다 가장 큰 확률의 인덱스(=class 번호)를 변수 preds에 저장\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds)) # 분류가 잘된 개수로 정확도 산출\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train(model, trainLoader, criterion, optimizer, use_cuda=True, weights_for_classes=None):\n",
    "    model.train()\n",
    "    runningLoss = 0.0\n",
    "    runningCorrects = 0\n",
    "    runningCorrectsEnsembleUnweighted = 0.0\n",
    "\n",
    "    for inputs, labels in trainLoader:\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs, out1x1, out2x1,out3x1,out4x1,out5x1 = model(x=inputs)\n",
    "            # 학습데이터에 대한 각 분류기의 정확도 결과를 반환\n",
    "            acc0 = acc(outputs, labels)\n",
    "            acc1 = acc(out1x1, labels)\n",
    "            acc2 = acc(out2x1, labels)\n",
    "            acc3 = acc(out3x1, labels)\n",
    "            acc4 = acc(out4x1, labels)\n",
    "            acc5 = acc(out5x1, labels)\n",
    "\n",
    "            # 각 분류기의 정확도를 통해 가중치 계산\n",
    "            w_for_classifier_sum = acc0+acc1+acc2+acc3+acc4+acc5\n",
    "            w_for_classifier0 = acc0/w_for_classifier_sum\n",
    "            w_for_classifier1 = acc1/w_for_classifier_sum\n",
    "            w_for_classifier2 = acc2/w_for_classifier_sum\n",
    "            w_for_classifier3 = acc3/w_for_classifier_sum\n",
    "            w_for_classifier4 = acc4/w_for_classifier_sum\n",
    "            w_for_classifier5 = acc5/w_for_classifier_sum\n",
    "\n",
    "            \n",
    "            w_for_instances = torch.zeros(outputs.size(0), 1)\n",
    "            if use_cuda:\n",
    "                w_for_instances = w_for_instances.cuda()\n",
    "            #    weights_for_classes = weights_for_classes.cuda()\n",
    "\n",
    "            weights_for_classes = None\n",
    "            \n",
    "            \n",
    "            loss1, w_for_instances = criterion(out1x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            loss2, w_for_instances = criterion(out2x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            loss3, w_for_instances = criterion(out3x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            loss4, w_for_instances = criterion(out4x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            loss5, w_for_instances = criterion(out5x1, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "\n",
    "            loss0, w_for_instances = criterion(outputs, labels.long(), w_for_instances, weight=weights_for_classes)\n",
    "            \n",
    "            loss = loss0+loss1+loss2+loss3+loss4+loss5\n",
    "\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, preds1 = torch.max(out1x1, 1)\n",
    "            _, preds2 = torch.max(out2x1, 1)\n",
    "            _, preds3 = torch.max(out3x1, 1)\n",
    "            _, preds4 = torch.max(out4x1, 1)\n",
    "            _, preds5 = torch.max(out5x1, 1)\n",
    "            \n",
    "            outputsEnsemble_unweighted = torch.zeros(outputs.size())\n",
    "\n",
    "            for i in range(inputs.size(0)):\n",
    "                outputsEnsemble_unweighted[i, preds[i]] = outputsEnsemble_unweighted[i, preds[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds1[i]] = outputsEnsemble_unweighted[i, preds1[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds2[i]] = outputsEnsemble_unweighted[i, preds2[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds3[i]] = outputsEnsemble_unweighted[i, preds3[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds4[i]] = outputsEnsemble_unweighted[i, preds4[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds5[i]] = outputsEnsemble_unweighted[i, preds5[i]] + 1\n",
    "            \n",
    "            _, predsEnsembleUnweighted = torch.max(outputsEnsemble_unweighted, 1)\n",
    "            \n",
    "            if use_cuda:\n",
    "                runningCorrectsEnsembleUnweighted = runningCorrectsEnsembleUnweighted+ torch.sum(predsEnsembleUnweighted.cuda()==labels.data).cpu()\n",
    "            else:\n",
    "                runningCorrectsEnsembleUnweighted = runningCorrectsEnsembleUnweighted+ torch.sum(predsEnsembleUnweighted==labels.data)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "        runningLoss += loss.item() * inputs.size(0)\n",
    "        runningCorrects = runningCorrects + torch.sum(preds == labels.data)\n",
    "\n",
    "        \n",
    "        overallLoss = [w_for_classifier0,w_for_classifier1,w_for_classifier2,w_for_classifier3,w_for_classifier4,w_for_classifier5]\n",
    "        if use_cuda:\n",
    "            model.module.setWeights(overallLoss)\n",
    "        else:\n",
    "            model.setWeights(overallLoss)\n",
    "    \n",
    "    epochLoss = runningLoss / len(trainLoader.dataset)\n",
    "    epochAcc = runningCorrects.double() / len(trainLoader.dataset)\n",
    "    epochAccEnsembleUnweighted = runningCorrectsEnsembleUnweighted.double() / len(trainLoader.dataset)\n",
    "    print('Train Loss: {:.4f} Acc:{:.4f} EnsembleAcc:{:.4f}'.format(epochLoss, epochAcc, epochAccEnsembleUnweighted))\n",
    "    return (epochLoss, epochAcc, epochAccEnsembleUnweighted)\n",
    "\n",
    "#모델 테스트 함수 정의\n",
    "def validate(model, testLoader, criterion, optimizer, use_cuda=True):\n",
    "    #print('test on validation')\n",
    "    model.eval()\n",
    "    runningLoss = 0.0\n",
    "    runningCorrects = 0.0\n",
    "   \n",
    "    runningCorrectsEnsemble=0.0\n",
    "    runningCorrectsEnsembleUnweighted = 0.0\n",
    "    runningCorrectsEnsemble_plus = 0.0\n",
    "    runningCorrectsEnsembleUnweighted_plus = 0.0\n",
    "    if use_cuda:\n",
    "        w = model.module.getWeights()\n",
    "    else:\n",
    "        w = model.getWeights()\n",
    "    #w = [1,1,1,1,1,1]\n",
    "    for inputs, labels in testLoader:\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            optimizer.zero_grad()\n",
    "            outputs,out1x1,out2x1,out3x1,out4x1,out5x1 = model(x=inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, preds1 = torch.max(out1x1, 1)\n",
    "            _, preds2 = torch.max(out2x1, 1)\n",
    "            _, preds3 = torch.max(out3x1, 1)\n",
    "            _, preds4 = torch.max(out4x1, 1)\n",
    "            _, preds5 = torch.max(out5x1, 1)\n",
    "            \n",
    "            outputsEnsemble = torch.zeros(outputs.size())\n",
    "            outputsEnsemble_unweighted = torch.zeros(outputs.size())\n",
    "\n",
    "            outputEnsemble_plus = outputs*w[0]+out1x1*w[1]+out2x1*w[2]+out3x1*w[3]+out4x1*w[4]+out5x1*w[5]\n",
    "            outputEnsemble_unweighted_plus = outputs+out1x1+out2x1+out3x1+out4x1+out5x1\n",
    "\n",
    "            for i in range(inputs.size(0)):\n",
    "                outputsEnsemble[i, preds[i]] = outputsEnsemble[i, preds[i]] + w[0]\n",
    "                outputsEnsemble[i, preds1[i]] = outputsEnsemble[i, preds1[i]] + w[1]\n",
    "                outputsEnsemble[i, preds2[i]] = outputsEnsemble[i, preds2[i]] + w[2]\n",
    "                outputsEnsemble[i, preds3[i]] = outputsEnsemble[i, preds3[i]] + w[3]\n",
    "                outputsEnsemble[i, preds4[i]] = outputsEnsemble[i, preds4[i]] + w[4]\n",
    "                outputsEnsemble[i, preds5[i]] = outputsEnsemble[i, preds5[i]] + w[5]\n",
    "\n",
    "                outputsEnsemble_unweighted[i, preds[i]] = outputsEnsemble_unweighted[i, preds[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds1[i]] = outputsEnsemble_unweighted[i, preds1[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds2[i]] = outputsEnsemble_unweighted[i, preds2[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds3[i]] = outputsEnsemble_unweighted[i, preds3[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds4[i]] = outputsEnsemble_unweighted[i, preds4[i]] + 1\n",
    "                outputsEnsemble_unweighted[i, preds5[i]] = outputsEnsemble_unweighted[i, preds5[i]] + 1\n",
    "            \n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, predsEnsemble = torch.max(outputsEnsemble, 1)\n",
    "            _, predsEnsembleUnweighted = torch.max(outputsEnsemble_unweighted, 1)\n",
    "            _, predsEnsemble_plus = torch.max(outputEnsemble_plus, 1)\n",
    "            _, predsEnsembleUnweighted_plus = torch.max(outputEnsemble_unweighted_plus, 1)\n",
    "            \n",
    "            if use_cuda:\n",
    "                runningCorrects = runningCorrects + torch.sum(preds==labels.data).cpu()\n",
    "                runningCorrectsEnsemble = runningCorrectsEnsemble + torch.sum(predsEnsemble.cuda()==labels.data).cpu()\n",
    "                runningCorrectsEnsembleUnweighted = runningCorrectsEnsembleUnweighted+ torch.sum(predsEnsembleUnweighted.cuda()==labels.data).cpu()\n",
    "                runningCorrectsEnsemble_plus = runningCorrectsEnsemble_plus + torch.sum(predsEnsemble_plus.cuda()==labels.data).cpu()\n",
    "                runningCorrectsEnsembleUnweighted_plus = runningCorrectsEnsembleUnweighted_plus+ torch.sum(predsEnsembleUnweighted_plus.cuda()==labels.data).cpu()\n",
    "            else:\n",
    "                runningCorrects = runningCorrects + torch.sum(preds==labels.data)\n",
    "                runningCorrectsEnsemble = runningCorrectsEnsemble + torch.sum(predsEnsemble==labels.data)\n",
    "                runningCorrectsEnsembleUnweighted = runningCorrectsEnsembleUnweighted+ torch.sum(predsEnsembleUnweighted==labels.data)\n",
    "                runningCorrectsEnsemble_plus = runningCorrectsEnsemble_plus + torch.sum(predsEnsemble_plus==labels.data)\n",
    "                runningCorrectsEnsembleUnweighted_plus = runningCorrectsEnsembleUnweighted_plus+ torch.sum(predsEnsembleUnweighted_plus==labels.data)\n",
    "            \n",
    "    \n",
    "    epochAcc = runningCorrects.double() / len(testLoader.dataset)\n",
    "    epochAccEnsemble = runningCorrectsEnsemble.double() / len(testLoader.dataset)\n",
    "    epochAccEnsembleUnweighted = runningCorrectsEnsembleUnweighted.double() / len(testLoader.dataset)\n",
    "    epochAccEnsemble_plus = runningCorrectsEnsemble_plus.double() / len(testLoader.dataset)\n",
    "    epochAccEnsembleUnweighted_plus = runningCorrectsEnsembleUnweighted_plus.double() / len(testLoader.dataset)\n",
    "    \n",
    "    print('Acc:{:.4f}   EnsembleAcc:{:.4f}   EnsembleUnweightedAcc:{:.4f}   Ensemble_plus:{:.4f}  Ensemble_unweighted:{:.4f}'.format(epochAcc, epochAccEnsemble, epochAccEnsembleUnweighted, epochAccEnsemble_plus, epochAccEnsembleUnweighted_plus))\n",
    "    \n",
    "    return ( epochAcc, epochAccEnsembleUnweighted )\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', acc=100., epoch=1):\n",
    "    if os.path.exists(checkpoint)==False:\n",
    "        os.makedirs(checkpoint)\n",
    "\n",
    "    filename = \"Epoch_\"+str(epoch)+\"_\"+str(acc)+\"_checkpoint.pth.tar\"\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, lr, epoch, schedule):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 following schedule\"\"\"\n",
    "    if epoch in schedule:\n",
    "        lr *= 0.1\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def get_checkpoint(epoch, checkpoint='checkpoint'):\n",
    "    if os.path.exists(checkpoint)==False:\n",
    "        os.makedirs(checkpoint)\n",
    "    \n",
    "    filename = \"Epoch_\"+str(epoch)+\"_checkpoint.pth.tar\"\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    return filepath\n",
    "\n",
    "# 메인 실행 함수(불균형 비율, 데이터셋 변환 가능)\n",
    "def MyEnsemble(IR=10., Cifar10_used=True, step_imbalance=True, root=\"./Result\", fine_tunning = 0):\n",
    "    if Cifar10_used:\n",
    "        num_class = 10\n",
    "    else:\n",
    "        num_class = 100\n",
    "    \n",
    "    learningRate = 0.05\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "    batchSize = 100\n",
    "\n",
    "    start_epoch = 0\n",
    "    epochs = 100\n",
    "    schedule = [30, 60, 90, 100]\n",
    "    gamma = 2.0\n",
    "    if step_imbalance:\n",
    "        model_save_path = os.path.join(root, \"step_imbalance\", \"cifar\"+str(num_class), \"IR=\"+str(IR), 'MyEnsemble')\n",
    "    else:\n",
    "        model_save_path = os.path.join(root, 'long_tail', \"cifar\"+str(num_class), \"IR=\"+str(IR), str(gamma), 'MyEnsemble')\n",
    "    \n",
    "    best_prec1 = 0\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # create model\n",
    "    model = resnext47_16_64(num_classes=num_class) # 모델 지정 - resnext\n",
    "    criterion = FocalLoss() # 논문에서 제안한 loss 함수 지정\n",
    "    \n",
    "    if use_cuda:\n",
    "        #model.cuda()\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learningRate,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # 데이터셋에 불균형성 추가 (이미지 잘라내기, 뒤집기, 회전)\n",
    "    transformTrain = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4), # padding 후 동일 크기의 이미지를 무작위로 잘라냄\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    transformTest = transforms.Compose([\n",
    "            transforms.CenterCrop(32),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    if Cifar10_used:\n",
    "        trainSet = CIFAR10(root='./data', train=True, download=True, transform=transformTrain, step_imbalance=step_imbalance, IR=IR)\n",
    "        trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize,  shuffle=True, num_workers=4)\n",
    "        testSet = CIFAR10(root='./data', train=False, download=True, transform=transformTest)\n",
    "        testLoader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=True, num_workers=4)\n",
    "        weights_for_classes = trainSet.weight_for_classes\n",
    "    else:\n",
    "        trainSet = CIFAR100(root='./data', train=True, download=True, transform=transformTrain, step_imbalance=step_imbalance, IR=IR)\n",
    "        trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize,  shuffle=True, num_workers=4)\n",
    "        testSet = CIFAR100(root='./data', train=False, download=True, transform=transformTest)\n",
    "        testLoader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=True, num_workers=4)\n",
    "        weights_for_classes = trainSet.weight_for_classes\n",
    "    if fine_tunning>0:\n",
    "        resume = get_checkpoint(fine_tunning, model_save_path) # 이력 체크\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(resume, checkpoint['epoch']))\n",
    "            checkpoint = os.path.dirname(resume)\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "    \n",
    "    singleTrainAcc = []\n",
    "    singleTestAcc = []\n",
    "    ensembleTrainAcc = []\n",
    "    ensembleTestAcc = []\n",
    "    #training & evaluation\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        learningRate = adjust_learning_rate(optimizer, learningRate, epoch, schedule)\n",
    "        print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, learningRate))\n",
    "\n",
    "        # train for one epoch\n",
    "        # train_acc는 single 네트워크의 학습데이터셋 정확도, ensembleACC는 보조 분류기를 사용한 네트워크의 학습데이터셋 정확도\n",
    "        train_loss, train_acc, ensembleAcc = train(model, trainLoader, criterion, optimizer, use_cuda=use_cuda, weights_for_classes=weights_for_classes)\n",
    "        # singleAcc는 single 네트워크의 테스트데이터셋 정확도, prec1는 ensemble 네트워크의 테스트데이터셋 정확도\n",
    "        singleAcc, prec1 = validate(model, testLoader, criterion, optimizer, use_cuda=use_cuda)\n",
    "\n",
    "        singleTrainAcc.append(train_acc.cpu().numpy())\n",
    "        singleTestAcc.append(singleAcc.cpu().numpy())\n",
    "        ensembleTrainAcc.append(ensembleAcc.cpu().numpy())\n",
    "        ensembleTestAcc.append(prec1.cpu().numpy())\n",
    "\n",
    "        is_best = prec1 > best_prec1 # ensemble 네트워크의 정확도가 최적값인지의 여부\n",
    "        best_prec1 = max(prec1, best_prec1) # 현 시점에서의 정확도 최적값을 체크\n",
    "        if (epoch % 5)==0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': prec1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best, checkpoint=model_save_path, acc=prec1.cpu().numpy(), epoch=epoch+1)\n",
    "\n",
    "    print('Best accuracy:')\n",
    "    print(best_prec1) # 최종적인 ensemble 네트워크의 정확도 최적값 출력\n",
    "    print(singleTrainAcc) # 최종적인 single 네트워크의 학습 정확도 출력\n",
    "    print(singleTestAcc) # 최종적인 single 네트워크의 테스트 정확도 출력\n",
    "    print(ensembleTrainAcc) # 최종적인 ensemble 네트워크의 학습 정확도 출력\n",
    "    print(ensembleTestAcc) # 최종적인 ensemble 네트워크의 테스트 정확도 출력\n",
    "\n",
    "    return best_prec1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1557a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyEnsemble()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
